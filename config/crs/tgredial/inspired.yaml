# dataset
dataset: Inspired
tokenize:
  rec: bert
  conv: gpt2
# dataloader
context_truncate: 256
response_truncate: 30
item_truncate: 100
scale: 1
# model
rec_model: TGRec
conv_model: TGConv
hidden_dropout_prob: 0.2
initializer_range: 0.02
hidden_size: 50
max_history_items: 100
num_attention_heads: 1
attention_probs_dropout_prob: 0.2
hidden_act: gelu
num_hidden_layers: 2
# optim
rec:
  epoch: 1
  batch_size: 8
  optimizer:
    name: AdamW
    lr: !!float 1e-3
    weight_decay: !!float 0.0000
  lr_bert: !!float 1e-5
  early_stop: true
  impatience: 3
  stop_mode: max
conv:
  epoch: 1
  batch_size: 8
  gradient_clip: 1.0
  update_freq: 1
  optimizer:
    name: AdamW
    lr: !!float 1.5e-4
  lr_scheduler:
    name: TransformersLinearLR
    warmup_steps: 2000
  early_stop: true
  impatience: 3
  stop_mode: min
